{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql7XigQskun6"
      },
      "source": [
        "## Decoder-only architecture - GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIn6pI0kun7"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqrz1C4kun7"
      },
      "source": [
        "For this exercise, we will focus on **decoder-only models**, particularly **GPT-2** (Generative Pre-trained Transformer 2), a model designed for text generation.\n",
        "\n",
        "#### **Decoder-Only Models**\n",
        "Decoder-only models, like GPT-2, differ from encoder-decoder models in that they generate text in a **unidirectional (left-to-right)** manner. These models do not have an encoder to process the entire input at once. Instead, they use **autoregressive** generation, predicting the next token based on the preceding context.\n",
        "\n",
        "For the part of this lab, we will use `GPT2LMHeadModel`, a GPT-2 model with a language modeling head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyfA5mINkun8",
        "outputId": "1f4d1ec5-da33-4bfd-8576-e04d23c648a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "model_name = 'gpt2'\n",
        "\n",
        "# TODO: Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHPIujBBkun8"
      },
      "source": [
        "### 1. **Embeddings Layer**\n",
        "The embeddings layer creates initial representations of the input tokens, encoding both the meaning of the words and their positions within the sequence.\n",
        "\n",
        "- **Word Embeddings** (`wte`): Maps each of the 50,257 vocabulary tokens into a 768-dimensional vector space.\n",
        "- **Position Embeddings** (`wpe`): Adds positional information to the tokens using a learned embedding of size 1024 (representing the maximum sequence length) with 768 dimensions. This allows the model to understand the order of the tokens, as transformers do not have an inherent sense of position.\n",
        "\n",
        "### 2. **Transformer Block (Decoder)**\n",
        "The core of GPT-2 is composed of 12 identical **transformer blocks**, each consisting of several sub-components. These blocks are stacked to process the input text in a sequential, autoregressive manner.\n",
        "\n",
        "Each transformer block includes the following:\n",
        "\n",
        "- **Layer Normalization (ln_1, ln_2)**: Normalization is applied before both the attention mechanism and the feed-forward network to stabilize and speed up training. It ensures that the inputs to the layers have the same mean and variance, using learned parameters.\n",
        "\n",
        "- **Self-Attention Mechanism**\n",
        "\n",
        "  - **Self-Attention Mechanism (attn)**: GPT-2 uses masked self-attention to ensure that the model can only attend to tokens that have already been processed, preventing the model from \"seeing\" future tokens. This makes the model autoregressive.\n",
        "    \n",
        "    - **Query, Key, Value Projections**: The attention mechanism computes the **query (Q)**, **key (K)**, and **value (V)** vectors using **Conv1D layers** (`c_attn`). Notice that (1) when the kernel size is 1, the role of Conv1d is just to do a linear projection, and (2) that under the hood, the Conv1d used in HuggingFace is indeed just a linear projection (see code [here](https://github.com/huggingface/transformers/blob/53fad641cfdb5105e2470bcf3ef17ea8e25cc300/src/transformers/pytorch_utils.py#L87)). In other words, `c_attn` performs a linear transformation and covers the role of $W_q$, $W_k$, and $W_v$. All other operations using Conv1d will follow the same principle.\n",
        "\n",
        "    - **Attention Output**: The output of the attention layer is passed through another **Conv1D layer** (`c_proj`) to project it back to the original 768-dimensional space.\n",
        "\n",
        "- **Feed-Forward Neural Network (mlp)**\n",
        "Each transformer block also contains a fully connected feed-forward neural network that processes the output of the attention mechanism. This is done in two stages:\n",
        "\n",
        "  - **First Linear Transformation (`c_fc`)**: Expands the dimensionality from 768 to a larger intermediate size using a **Conv1D layer**.\n",
        "    \n",
        "  - **Activation Function (`act`)**: Applies the **GELU (Gaussian Error Linear Unit)** activation function, which introduces non-linearity and helps the model capture complex patterns in the data.\n",
        "\n",
        "  - **Second Linear Transformation (`c_proj`)**: Projects the output back down from the intermediate size to 768 dimensions using another **Conv1D layer**.\n",
        "\n",
        "- **Residual Connections**\n",
        "  - GPT-2 uses skip connections around both the self-attention and feed-forward layers, where the input to each sub-layer is added to its output.\n",
        "\n",
        "### 3. **Final Layer Normalization and Language Modeling Head**\n",
        "- After passing through all 12 transformer blocks, a final **LayerNorm** (`ln_f`) is applied to normalize the output before it is passed to the language modeling head for token prediction.\n",
        "\n",
        "- The output of the final transformer block is passed through a **linear layer** (`lm_head`) which maps the 768-dimensional hidden states to the vocabulary size (50,257). This step is essential for converting the hidden representations into predictions for the next token in the sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE349peBkun8"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the input prompt\n",
        "prompt = \"I had called upon my friend, Mr. Sherlock Holmes, one day in the autumn of last year and found him in deep conversation with a very stout, florid-faced, elderly gentleman with fiery red hair.\"\n",
        "\n",
        "# TODO: Tokenize the input prompt\n",
        "input_ids = ...\n",
        "\n",
        "# TODO: Autoregressively generate tokens (`max_length=200`, `do_sample=True`, `temperature=1.1`)\n",
        "generated_ids = ...\n",
        "\n",
        "# TODO: Decode and print the generated text\n",
        "generated_text = ...\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk6nQ7Yikun8"
      },
      "source": [
        "## 2. Decoder (Masked) Self-Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiR0q5cTkun9"
      },
      "source": [
        "In this exercise, we will focus on the **masked self-attention mechanism** used in the decoder blocks of GPT-2. This mechanism allows the model to attend to previous tokens in an autoregressive manner, ensuring that the model generates text sequentially from left to right without looking ahead to future tokens.\n",
        "\n",
        "A plot of the **self-attention matrix** for a single decoder block in GPT-2 is shown below. The matrix represents the attention weights between different tokens in the input sequence. The **x-axis** and **y-axis** correspond to the tokens in the input sequence. Each position on these axes represents a specific token in the input text.\n",
        "  \n",
        "#### **Masked Self-Attention Behavior:**\n",
        "- As expected in a **decoder-only** model, we observe a clear triangular pattern. Tokens only attend to themselves and the tokens that precede them. For example, the first token (`I`) only attends to itself, the second token (`love`) attends to both itself and the first token, and so on.\n",
        "- The upper-right part of the matrix is empty (dark purple), indicating that the model **masks future tokens** to prevent them from being used in generating the current token. This ensures that GPT-2 maintains its **autoregressive property**, where each token is generated based only on past tokens.\n",
        "\n",
        "- In the image, you can see that certain tokens attend more strongly to previous tokens. For example, the token `favorite` attends heavily to earlier tokens like `Italian` and `food`, as indicated by the brighter colors in the heatmap.\n",
        "- The model tends to attend more to the recent past tokens, which is crucial for maintaining context during text generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w7XlupTjkun9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "yjvQ-ENTkun9",
        "outputId": "30a0d063-03c4-44f1-8957-9075ebc35fc4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but got index is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-381499539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# TODO: Generate the model output and retrieve attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m  \u001b[0;31m# List of attention matrices from all layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcache_position\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2544\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got index is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "model_name = 'gpt2'\n",
        "model = GPT2Model.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Input prompt\n",
        "prompt = \"I love Italian food, my favorite dish is\"\n",
        "\n",
        "# TODO: Tokenize the input prompt\n",
        "inputs =  tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # TODO: Generate the model output and retrieve attention weights\n",
        "    outputs = model(**inputs)\n",
        "    attentions = outputs.attentions  # List of attention matrices from all layers\n",
        "\n",
        "# Visualize the attention weights from the last layer\n",
        "attention_matrix = attentions[-1][0][10].cpu().numpy()  # Last layer, tenth head\n",
        "\n",
        "# Get the tokens for the input prompt\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "# Plot the attention matrix (last layer, first head) with words as labels\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "plt.title('Self-Attention Matrix (Last Layer, Tenth Head)')\n",
        "plt.xlabel('Token')\n",
        "plt.ylabel('Token')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC78M9PYkun9"
      },
      "source": [
        "## 3. Positional Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-eYFPP4kun9"
      },
      "source": [
        "Similarly to what we have seen in the previous exercise (`01-bert`), GPT-2 uses **positional embeddings** to encode the position of tokens in the input sequence. These embeddings are added to the token embeddings to provide the model with information about the order of the tokens.\n",
        "\n",
        "Also in GPT-2, the positional embeddings are learned during training, allowing the model to capture complex patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5yFmSLakun9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q8YKO3Bkun9"
      },
      "outputs": [],
      "source": [
        "# Extract learned positional embeddings from the model\n",
        "positional_embeddings = model.wpe.weight.detach().cpu().numpy()  # Shape: (1024, 768)\n",
        "\n",
        "# TODO: Compute cosine similarity between positional embeddings\n",
        "cosine_sim = ...\n",
        "\n",
        "fig, ax = plt.subplot_mosaic(\"\"\"\n",
        "AAA\n",
        "AAA\n",
        "AAA\n",
        "BBB\"\"\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot the cosine similarity heatmap\n",
        "ax[\"A\"].set_aspect('equal')\n",
        "cbar = ax[\"A\"].imshow(cosine_sim, cmap='viridis', vmin=-1, vmax=1)\n",
        "ax[\"A\"].set_xlabel('Vector 1')\n",
        "ax[\"A\"].set_ylabel('Vector 2')\n",
        "ax[\"A\"].axhline(100, c='r', lw=2)\n",
        "# add colorbar\n",
        "fig.colorbar(cbar, ax=ax[\"A\"])\n",
        "\n",
        "\n",
        "# Plot the cosine similarity for a specific row\n",
        "\n",
        "ax[\"B\"].plot(cosine_sim[100], c='r', lw=2)\n",
        "ax[\"B\"].set_xlabel('Vector 1')\n",
        "ax[\"B\"].set_ylabel('Cosine Similarity')\n",
        "ax[\"B\"].grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC0-CT_8kun9"
      },
      "source": [
        "## 4. Sampling Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NWLcPGHkun9"
      },
      "source": [
        "### **Implementing Sampling Methods:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWp7cX7fkun9"
      },
      "source": [
        "In this exercise, we will explore various **sampling methods** used for text generation in GPT-2. Each sampling method determines how the model selects the next token when generating text.\n",
        "\n",
        "GPT-2 produces a probability distribution over possible tokens for each position in the sequence. The different sampling strategies allow us to choose how we sample from this distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "USAaIbVLkun-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IyooXs4Ikun-"
      },
      "outputs": [],
      "source": [
        "# TODO: Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Set the pad token to eos token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Input prompt\n",
        "prompt = \"I had called upon my friend, Mr. Sherlock Holmes, one day in the autumn of last year and found him in deep conversation with a very stout, florid-faced, elderly gentleman with fiery red hair.\"\n",
        "\n",
        "# TODO: Tokenize the input prompt\n",
        "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDuqQdb9kun-"
      },
      "source": [
        "1. **Greedy Sampling**: Always selects the token with the highest probability at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbxT2mvykun-"
      },
      "outputs": [],
      "source": [
        "# Greedy Sampling: Pick the token with the highest probability\n",
        "def greedy_sampling(model, input_ids, max_length):\n",
        "    input_ids = input_ids.to(device)\n",
        "    for _ in range(max_length):\n",
        "        # NOTE: There's no need to pass an attention mask, since we are passing a single\n",
        "        # input sequence and the model will automatically create the mask.\n",
        "        logits = model(input_ids=input_ids).logits[:, -1, :] # Compute the logits for the last token (1st batch)\n",
        "        next_token = torch.argmax(logits, dim=1).unsqueeze(1) # Get the most likely next token\n",
        "        input_ids = torch.hstack([input_ids, next_token]) # Add the token to the output sequence\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "# Generate and return text using each sampling method\n",
        "max_length = 100  # Max tokens to generate\n",
        "\n",
        "# Greedy Sampling\n",
        "input_length = inputs[\"input_ids\"].shape[-1]\n",
        "output_greedy = greedy_sampling(model, input_ids, max_length)\n",
        "print(prompt)\n",
        "print(tokenizer.decode(output_greedy[0, input_length:], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUakLwSEkun-"
      },
      "source": [
        "2. **Beam Search**: Expands multiple candidate sequences and selects the best based on their cumulative probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL6SnF-3kun-"
      },
      "outputs": [],
      "source": [
        "# Beam Search (simple version with fixed beam width)\n",
        "\n",
        "# NOTE: each score is the log-probability of the sequence\n",
        "# (remember, the sum of log-probabilities is the log-probability of the product)\n",
        "# (i.e., of the entire sentence)\n",
        "def beam_search(model, input_ids, max_length, beam_width=3):\n",
        "    sequences = [(input_ids.to(device), 0)]  # (sequence, score)\n",
        "    for _ in range(max_length):\n",
        "        all_candidates = []\n",
        "        for seq, mask, score in sequences:\n",
        "            #TODO: Compute the logits\n",
        "            logits = ...\n",
        "\n",
        "            #TODO: Compute the top k tokens and their probabilities\n",
        "            probs = ...\n",
        "            top_k_probs, top_k_tokens = torch.topk(probs, beam_width, dim=-1) # Get top k tokens\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                candidate_seq = torch.cat([seq, top_k_tokens[:, i].unsqueeze(-1)], dim=-1)  # Add token to sequence\n",
        "                candidate_mask = torch.cat([mask, torch.ones((1, 1), dtype=torch.long)], dim=-1)\n",
        "                candidate = (candidate_seq, candidate_mask, score - torch.log(top_k_probs[:, i]).item()) # Update score\n",
        "                all_candidates.append(candidate) # Add new candidate\n",
        "\n",
        "        # Select top `beam_width` sequences\n",
        "        sequences = sorted(all_candidates, key=lambda x: x[2])[:beam_width]\n",
        "\n",
        "        if sequences[0][0][0, -1].item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return sequences[0][0]\n",
        "\n",
        "# Beam Search\n",
        "output_beam = beam_search(model, input_ids, max_length, beam_width=5)\n",
        "print(prompt)\n",
        "print(tokenizer.decode(output_beam[0, input_length:], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g3cmD6rkun-"
      },
      "source": [
        "3. **Random Sampling**: Samples tokens randomly according to their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YNJQhM4kun-"
      },
      "outputs": [],
      "source": [
        "# Random Sampling: Randomly sample a token from the distribution\n",
        "def random_sampling(model, input_ids, max_length):\n",
        "    output = input_ids\n",
        "    for _ in range(max_length):\n",
        "        #TODO: Compute the logits\n",
        "        logits = ...\n",
        "\n",
        "        #TODO: Sample a token from the probability distribution (hint: use torch.multinomial)\n",
        "        probs = ...\n",
        "        next_token = ...\n",
        "\n",
        "        #TODO: Update the output and attention mask\n",
        "        output = ...\n",
        "        attention_mask = ...\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return output\n",
        "\n",
        "# Random Sampling\n",
        "output_random = random_sampling(model, input_ids, max_length)\n",
        "print(prompt)\n",
        "print(tokenizer.decode(output_random[0, input_length:], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mpi7nhgkun-"
      },
      "source": [
        "4. **Top-k Sampling**: Samples only from the top `k` most probable tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyG8k7D_kun-"
      },
      "outputs": [],
      "source": [
        "# Top-k Sampling: Sample from the top k tokens\n",
        "def top_k_sampling(model, input_ids, max_length, k=50):\n",
        "    output = input_ids\n",
        "    for _ in range(max_length):\n",
        "        #TODO: Compute the logits\n",
        "        logits = ...\n",
        "\n",
        "        #TODO: Sample a token from the top k tokens (hint: use torch.topk and torch.multinomial)\n",
        "        top_k_logits, top_k_tokens = ...\n",
        "        top_k_probs = ...\n",
        "        sampled_index = ...\n",
        "        next_token = ... # Select the sampled token\n",
        "\n",
        "        #TODO: Update the output and attention mask\n",
        "        output = ...\n",
        "        attention_mask = ...\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return output\n",
        "\n",
        "# Top-k Sampling\n",
        "output_top_k = top_k_sampling(model, input_ids, max_length, k=50)\n",
        "print(prompt)\n",
        "print(tokenizer.decode(output_top_k[0, input_length:], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMBJtW9hkun-"
      },
      "source": [
        "5. **Top-p (Nucleus) Sampling**: Samples from the smallest set of tokens whose cumulative probability exceeds `p`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvzOR5GNkun_"
      },
      "outputs": [],
      "source": [
        "# Top-p (Nucleus) Sampling: Sample from the smallest set of tokens whose cumulative probability exceeds p\n",
        "def top_p_sampling(model, input_ids, max_length, p=0.9):\n",
        "    output = input_ids\n",
        "    for _ in range(max_length):\n",
        "        #TODO: Compute the logits\n",
        "        logits = ...\n",
        "\n",
        "        # TODO: create a mask to filter out tokens with cumulative probability above p (hint: use torch.sort and torch.cumsum)\n",
        "        sorted_logits, sorted_indices = ...\n",
        "        cumulative_probs = ... # Cumulative Distribution Function (CDF)\n",
        "        top_p_indices = cumulative_probs <= p\n",
        "\n",
        "        if top_p_indices.sum() == 0:  # Fix for when there are no valid top-p indices\n",
        "            top_p_indices[0] = True  # Ensure at least one token is considered\n",
        "\n",
        "        #TODO: Recompute the probabilities and sample from the updated distribution\n",
        "        top_p_probs = ... # Recompute the probabilities\n",
        "        sampled_index = ... # Sample from the updated distribution\n",
        "        next_token = ...\n",
        "\n",
        "        #TODO: Update the output and attention mask\n",
        "        output = ...\n",
        "        attention_mask = ...\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return output\n",
        "\n",
        "# Top-p (Nucleus) Sampling\n",
        "output_top_p = top_p_sampling(model, input_ids, max_length, p=0.9)\n",
        "print(prompt)\n",
        "print(tokenizer.decode(output_top_p[0, input_length:], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1aF09hikun_"
      },
      "outputs": [],
      "source": [
        "len(input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FoAqK_kun_"
      },
      "source": [
        "6. **Temperature Scaling**: Adjusts the randomness of the token selection by scaling the probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIj1ARmhkun_"
      },
      "outputs": [],
      "source": [
        "# Temperature Scaling: Adjust the randomness of predictions by scaling the logits\n",
        "def temperature_sampling(model, input_ids, max_length, temperature=0.7):\n",
        "    output = input_ids\n",
        "    for _ in range(max_length):\n",
        "        #TODO: Compute the logits\n",
        "        logits =...\n",
        "\n",
        "        #TODO: Scale the logits by the temperature parameter and sample from the updated distribution\n",
        "        logits = ... # Scale the logits by the temperature parameter\n",
        "        probs = ... # Compute the probabilities\n",
        "        next_token = ... # Sample from the updated distribution\n",
        "\n",
        "        #TODO: Update the output and attention mask\n",
        "        output = ...\n",
        "        attention_mask = ...\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return output\n",
        "\n",
        "# Temperature Sampling\n",
        "print(prompt)\n",
        "print()\n",
        "print(\"Temperature 0.25\")\n",
        "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=0.25)\n",
        "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
        "print()\n",
        "print(\"Temperature 0.5\")\n",
        "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=0.5)\n",
        "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
        "print()\n",
        "print(\"Temperature 0.75\")\n",
        "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=0.75)\n",
        "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
        "print()\n",
        "print(\"Temperature 1.1\")\n",
        "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=1.1)\n",
        "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
        "print()\n",
        "print(\"Temperature 1.5\")\n",
        "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=1.5)\n",
        "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHN2azmgkun_"
      },
      "source": [
        "### **Using Pre-Implemented Sampling Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwjsYQFCkun_"
      },
      "source": [
        "The Hugging Face `transformers` library offers built-in implementations for a variety of sampling methods used in text generation , such as the one we have just implemented, through the `generate()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYqfJfJzkun_",
        "outputId": "d49b0c98-c547-48c6-854a-2c7de4ddf452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy Sampling:\n",
            " He was a very handsome man, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed, and I was very glad to see him. He was very well dressed\n",
            "\n",
            "Beam Search:\n",
            " He told me that he had come to see me, and that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time in a long time. He told me that he had come to see me for the first time\n",
            "\n",
            "Random Sampling:\n",
            " The young man's own complexion was such that it was evident to the curious eye that he had been wearing a very strong robe. He then asked us for a glass, which we gladly gladly requested. In the course of a moment, he placed an open glass on our backs, gave it a shake and said, \"You need some hot water,\" and we, being in the midst of the conversation, took him into our midst and looked him in the eye.\n",
            "\n",
            "The gentleman made a very good impression on me and expressed his joyous appreciation. He then pointed his wand at my heart and said in a tone of sincere admiration, \"You can only be at the present day as the only present of your kind, but I have no doubt it is my duty to you\n",
            "\n",
            "Top-50 Sampling:\n",
            " He felt I had some bad feeling on him but decided to turn over my own papers. I told him I came here from Liverpool to-day, just over the Thames and we were looking for a pub. I took my leave of him, having been at the pub for a while, and did not have the decency to ask for the papers. He told me we had been in Liverpool for a while and had looked for one as soon as we returned to Ireland and went to see the local pub as soon as we arrived in Dorset. I told him we would continue looking for him till we got on a Sunday and found him so thoroughly and so well behaved, and seemed not to care to make any sort of complaints against him. He never bothered to tell me where he\n",
            "\n",
            "Top-0.9 (Nucleus) Sampling:\n",
            " He talked for a few minutes, and then gave me the story, which he was very fond of, which I told him that I should be delighted to read. I then went over to the door and sat down, but Mr. Holmes came in, and I found him staring at me, looking at the clock with a big expression of admiration upon his face, and a cold smirk on his lips. I sat down, and then Mr. Holmes asked me what he had come for. \"I know you must be quite tired,\" I replied. \"I must be up at the office tomorrow, as I've got some other business to do in the afternoon.\" He shook his head and said, \"Well, what do you think, Mr. Holmes?\" \"Oh,\n",
            "\n",
            "Temperature Sampling (0.7):\n",
            " I was then asked what he was thinking of, and whether he was going to write me a letter. He replied, \"We've been talking about something, you can't do that. You're not going to publish it, you can't do it. Look at the facts. It's not about me. That's what it is. It's the facts.\"\n",
            "\n",
            "This gentleman, who had been visiting me at the time, had always been very kind, and had always had a good sense of humour. He never took no interest in me, and he was a very good man indeed. He was my very best friend and companion and, at the same time, I always loved him.\n",
            "\n",
            "The gentleman, who had come to visit me at the time,\n"
          ]
        }
      ],
      "source": [
        "# Function to generate text using various sampling methods\n",
        "def generate_text(model, input_ids, max_length, method, **kwargs):\n",
        "\n",
        "    methods = {\n",
        "        \"greedy\": {},\n",
        "        \"beam\": {\"num_beams\": kwargs.get('num_beams', 3), \"early_stopping\": True},\n",
        "        \"random\": {\"do_sample\": True},\n",
        "        \"top_k\": {\"do_sample\": True, \"top_k\": kwargs.get('top_k', 50)},\n",
        "        \"top_p\": {\"do_sample\": True, \"top_p\": kwargs.get('top_p', 0.9)},\n",
        "        \"temperature\": {\"do_sample\": True, \"temperature\": kwargs.get('temperature', 0.7)}\n",
        "    }\n",
        "    return model.generate(input_ids.to(device), max_length=max_length, pad_token_id=tokenizer.eos_token_id, **methods[method])\n",
        "\n",
        "# Generate text using each sampling method\n",
        "max_length = 200  # Increased max_length to generate more tokens\n",
        "\n",
        "# Get the length of the input prompt in tokens\n",
        "input_length = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
        "\n",
        "# Greedy Sampling\n",
        "output_greedy = generate_text(model, inputs['input_ids'], max_length, 'greedy')\n",
        "print(\"Greedy Sampling:\")\n",
        "print(tokenizer.decode(output_greedy[0, input_length:], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "# Beam Search\n",
        "output_beam = generate_text(model, inputs['input_ids'], max_length, 'beam', num_beams=5)\n",
        "print(\"\\nBeam Search:\")\n",
        "print(tokenizer.decode(output_beam[0, input_length:], skip_special_tokens=True))\n",
        "\n",
        "# Random Sampling\n",
        "output_random = generate_text(model, inputs['input_ids'], max_length, 'random')\n",
        "print(\"\\nRandom Sampling:\")\n",
        "print(tokenizer.decode(output_random[0, input_length:], skip_special_tokens=True))\n",
        "\n",
        "# Top-k Sampling\n",
        "k = 50\n",
        "output_top_k = generate_text(model, inputs['input_ids'], max_length, 'top_k', top_k=k)\n",
        "print(f\"\\nTop-{k} Sampling:\")\n",
        "print(tokenizer.decode(output_top_k[0, input_length:], skip_special_tokens=True))\n",
        "\n",
        "# Top-p (Nucleus) Sampling\n",
        "p = 0.9\n",
        "output_top_p = generate_text(model, inputs['input_ids'], max_length, 'top_p', top_p=p)\n",
        "print(f\"\\nTop-{p} (Nucleus) Sampling:\")\n",
        "print(tokenizer.decode(output_top_p[0, input_length:], skip_special_tokens=True))\n",
        "\n",
        "# Temperature Scaling\n",
        "temperature = 0.7\n",
        "output_temperature = generate_text(model, inputs['input_ids'], max_length, 'temperature', temperature=temperature)\n",
        "print(f\"\\nTemperature Sampling ({temperature}):\")\n",
        "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}